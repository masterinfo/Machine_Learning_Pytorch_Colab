{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_Optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masterinfo/Machine_Learning_Pytorch_Colab/blob/main/Using_Optimizers_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy"
      },
      "source": [
        "# Using optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS"
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chargement des datas et normalisation"
      ],
      "metadata": {
        "id": "VwvaQlUZLO7e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZpZ12MrEDZI"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "on ne se sers pas  de la classe"
      ],
      "metadata": {
        "id": "loFnh6ODLWXh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH"
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "#model = FMNIST()   "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m68OeMRdEF0X"
      },
      "source": [
        ""
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creation du model a 3 couches  784 entrÃ©s ( le spixels) =>128 neurones premmiere couche avec un un fonction d'activation Relu )=> 64 neurones sur la seconde couche avec une fonction d'activation Relu => couche de sortie de 10 neurones avec logsoftmax multiclasse"
      ],
      "metadata": {
        "id": "upV_9ux0LaYf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-"
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjBut_7lhAc8"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZAGFzFEQA_"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu"
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnVwV-CERd_"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbHIyPSEUPh"
      },
      "source": [
        ""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtP3nCEQEUMH"
      },
      "source": [
        ""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwcPkxQwEfYX"
      },
      "source": [
        ""
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "premier passage juste pour comprendre l'effet de l'optimizer"
      ],
      "metadata": {
        "id": "lTiTWRxZMhe8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be08397-a27d-4349-e4e1-f4a0d8ca758a"
      },
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n",
        "\n",
        "        "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "update the weights_grad ( calculs des gradients )"
      ],
      "metadata": {
        "id": "IyZAPoQOMsUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWFf4Y9dQPfS",
        "outputId": "24b73db3-50a9-4f7a-c480-a3a05bb43910"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updat the weights ( neurones weights )"
      ],
      "metadata": {
        "id": "NXkIiWdQRI0U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e2815a4-b3db-4797-b682-3f293cd6233e"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[ 0.0012,  0.0206, -0.0279,  ...,  0.0234,  0.0052,  0.0035],\n",
            "        [-0.0209, -0.0161, -0.0116,  ..., -0.0215, -0.0071, -0.0311],\n",
            "        [-0.0202,  0.0148, -0.0334,  ..., -0.0204,  0.0011,  0.0079],\n",
            "        ...,\n",
            "        [ 0.0011, -0.0302,  0.0078,  ..., -0.0044,  0.0029,  0.0293],\n",
            "        [-0.0243, -0.0231, -0.0075,  ...,  0.0105, -0.0334, -0.0168],\n",
            "        [ 0.0290,  0.0047,  0.0107,  ...,  0.0267,  0.0331, -0.0124]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
            "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0024,  0.0022,  0.0022],\n",
            "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
            "        ...,\n",
            "        [ 0.0014,  0.0014,  0.0014,  ...,  0.0014,  0.0014,  0.0014],\n",
            "        [ 0.0021,  0.0021,  0.0021,  ...,  0.0022,  0.0021,  0.0021],\n",
            "        [ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reset des gradients..( avant nouveau passage )"
      ],
      "metadata": {
        "id": "iIE9qH18RTFr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfpzGigEpAr"
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46da637d-206c-41f9-e3f9-0f5f9de4f93c"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[ 0.0012,  0.0206, -0.0279,  ...,  0.0234,  0.0052,  0.0035],\n",
            "        [-0.0209, -0.0161, -0.0116,  ..., -0.0215, -0.0071, -0.0311],\n",
            "        [-0.0202,  0.0148, -0.0334,  ..., -0.0204,  0.0011,  0.0079],\n",
            "        ...,\n",
            "        [ 0.0011, -0.0302,  0.0078,  ..., -0.0044,  0.0029,  0.0293],\n",
            "        [-0.0243, -0.0231, -0.0075,  ...,  0.0105, -0.0334, -0.0168],\n",
            "        [ 0.0290,  0.0047,  0.0107,  ...,  0.0267,  0.0331, -0.0124]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DViAViGEwyr"
      },
      "source": [
        ""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "application qui repars depuis le debut"
      ],
      "metadata": {
        "id": "Xpmg2SyCPS8r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd41bac7-5c26-49ab-94eb-22253faa3a7c"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "       \n",
        "     \n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 1.0100424232513412\n",
            "Training loss: 0.5585998177909648\n",
            "Training loss: 0.4902866960906271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**how to see loss for each batch of 1 epoch**"
      ],
      "metadata": {
        "id": "G589YxPASHP0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVDFUnzFGpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a196980-49c7-479b-be58-4d6b1d7b94a0"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_num=0\n",
        "\n",
        "    for batch_num,(images, labels) in enumerate(trainloader,1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        print(f'Batch : {batch_num}, Loss : {loss.item()}')\n",
        "     \n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch : 1, Loss : 2.3215713500976562\n",
            "Batch : 2, Loss : 2.3074803352355957\n",
            "Batch : 3, Loss : 2.3164830207824707\n",
            "Batch : 4, Loss : 2.297412157058716\n",
            "Batch : 5, Loss : 2.2926530838012695\n",
            "Batch : 6, Loss : 2.2953672409057617\n",
            "Batch : 7, Loss : 2.3005030155181885\n",
            "Batch : 8, Loss : 2.2910714149475098\n",
            "Batch : 9, Loss : 2.2748677730560303\n",
            "Batch : 10, Loss : 2.2863876819610596\n",
            "Batch : 11, Loss : 2.268754720687866\n",
            "Batch : 12, Loss : 2.268362522125244\n",
            "Batch : 13, Loss : 2.286766529083252\n",
            "Batch : 14, Loss : 2.2703473567962646\n",
            "Batch : 15, Loss : 2.2913095951080322\n",
            "Batch : 16, Loss : 2.2611446380615234\n",
            "Batch : 17, Loss : 2.2757766246795654\n",
            "Batch : 18, Loss : 2.27260160446167\n",
            "Batch : 19, Loss : 2.27754807472229\n",
            "Batch : 20, Loss : 2.273784875869751\n",
            "Batch : 21, Loss : 2.230733633041382\n",
            "Batch : 22, Loss : 2.2553842067718506\n",
            "Batch : 23, Loss : 2.2328648567199707\n",
            "Batch : 24, Loss : 2.255995035171509\n",
            "Batch : 25, Loss : 2.2610247135162354\n",
            "Batch : 26, Loss : 2.240332841873169\n",
            "Batch : 27, Loss : 2.235386848449707\n",
            "Batch : 28, Loss : 2.2434561252593994\n",
            "Batch : 29, Loss : 2.2051637172698975\n",
            "Batch : 30, Loss : 2.2206692695617676\n",
            "Batch : 31, Loss : 2.242758274078369\n",
            "Batch : 32, Loss : 2.226162910461426\n",
            "Batch : 33, Loss : 2.244436740875244\n",
            "Batch : 34, Loss : 2.231757402420044\n",
            "Batch : 35, Loss : 2.2038097381591797\n",
            "Batch : 36, Loss : 2.1911041736602783\n",
            "Batch : 37, Loss : 2.1948752403259277\n",
            "Batch : 38, Loss : 2.1990513801574707\n",
            "Batch : 39, Loss : 2.214055061340332\n",
            "Batch : 40, Loss : 2.191995143890381\n",
            "Batch : 41, Loss : 2.1939537525177\n",
            "Batch : 42, Loss : 2.1965179443359375\n",
            "Batch : 43, Loss : 2.2005298137664795\n",
            "Batch : 44, Loss : 2.181551694869995\n",
            "Batch : 45, Loss : 2.1860036849975586\n",
            "Batch : 46, Loss : 2.164980411529541\n",
            "Batch : 47, Loss : 2.167433023452759\n",
            "Batch : 48, Loss : 2.163151979446411\n",
            "Batch : 49, Loss : 2.1516082286834717\n",
            "Batch : 50, Loss : 2.1331982612609863\n",
            "Batch : 51, Loss : 2.1778361797332764\n",
            "Batch : 52, Loss : 2.1359477043151855\n",
            "Batch : 53, Loss : 2.134805202484131\n",
            "Batch : 54, Loss : 2.1352760791778564\n",
            "Batch : 55, Loss : 2.143667697906494\n",
            "Batch : 56, Loss : 2.1367557048797607\n",
            "Batch : 57, Loss : 2.1460916996002197\n",
            "Batch : 58, Loss : 2.1325337886810303\n",
            "Batch : 59, Loss : 2.1505327224731445\n",
            "Batch : 60, Loss : 2.083942174911499\n",
            "Batch : 61, Loss : 2.0953426361083984\n",
            "Batch : 62, Loss : 2.0992610454559326\n",
            "Batch : 63, Loss : 2.1095194816589355\n",
            "Batch : 64, Loss : 2.0754876136779785\n",
            "Batch : 65, Loss : 2.073982000350952\n",
            "Batch : 66, Loss : 2.132622003555298\n",
            "Batch : 67, Loss : 2.099835157394409\n",
            "Batch : 68, Loss : 2.043778896331787\n",
            "Batch : 69, Loss : 2.0729687213897705\n",
            "Batch : 70, Loss : 2.0938217639923096\n",
            "Batch : 71, Loss : 2.10217547416687\n",
            "Batch : 72, Loss : 2.056648015975952\n",
            "Batch : 73, Loss : 2.081895112991333\n",
            "Batch : 74, Loss : 2.0219688415527344\n",
            "Batch : 75, Loss : 2.059443950653076\n",
            "Batch : 76, Loss : 2.068291425704956\n",
            "Batch : 77, Loss : 2.054886817932129\n",
            "Batch : 78, Loss : 2.02876615524292\n",
            "Batch : 79, Loss : 2.0386362075805664\n",
            "Batch : 80, Loss : 1.9955843687057495\n",
            "Batch : 81, Loss : 1.9903777837753296\n",
            "Batch : 82, Loss : 1.9961559772491455\n",
            "Batch : 83, Loss : 2.0275208950042725\n",
            "Batch : 84, Loss : 1.9773322343826294\n",
            "Batch : 85, Loss : 1.966648817062378\n",
            "Batch : 86, Loss : 1.9854862689971924\n",
            "Batch : 87, Loss : 1.9483871459960938\n",
            "Batch : 88, Loss : 1.979067087173462\n",
            "Batch : 89, Loss : 1.9612038135528564\n",
            "Batch : 90, Loss : 1.9842612743377686\n",
            "Batch : 91, Loss : 1.9231582880020142\n",
            "Batch : 92, Loss : 1.95497727394104\n",
            "Batch : 93, Loss : 1.9392272233963013\n",
            "Batch : 94, Loss : 1.8900784254074097\n",
            "Batch : 95, Loss : 1.9706110954284668\n",
            "Batch : 96, Loss : 1.8864530324935913\n",
            "Batch : 97, Loss : 1.9499945640563965\n",
            "Batch : 98, Loss : 1.8619617223739624\n",
            "Batch : 99, Loss : 1.9457640647888184\n",
            "Batch : 100, Loss : 1.9349318742752075\n",
            "Batch : 101, Loss : 1.9444459676742554\n",
            "Batch : 102, Loss : 1.9407682418823242\n",
            "Batch : 103, Loss : 1.8843700885772705\n",
            "Batch : 104, Loss : 1.907165765762329\n",
            "Batch : 105, Loss : 1.885127067565918\n",
            "Batch : 106, Loss : 1.838998556137085\n",
            "Batch : 107, Loss : 1.8788009881973267\n",
            "Batch : 108, Loss : 1.8974528312683105\n",
            "Batch : 109, Loss : 1.8449511528015137\n",
            "Batch : 110, Loss : 1.8473812341690063\n",
            "Batch : 111, Loss : 1.8380472660064697\n",
            "Batch : 112, Loss : 1.8186711072921753\n",
            "Batch : 113, Loss : 1.8119462728500366\n",
            "Batch : 114, Loss : 1.829735517501831\n",
            "Batch : 115, Loss : 1.7757610082626343\n",
            "Batch : 116, Loss : 1.799276351928711\n",
            "Batch : 117, Loss : 1.7289690971374512\n",
            "Batch : 118, Loss : 1.6927753686904907\n",
            "Batch : 119, Loss : 1.8219108581542969\n",
            "Batch : 120, Loss : 1.811837077140808\n",
            "Batch : 121, Loss : 1.7215633392333984\n",
            "Batch : 122, Loss : 1.7490637302398682\n",
            "Batch : 123, Loss : 1.7260392904281616\n",
            "Batch : 124, Loss : 1.8227908611297607\n",
            "Batch : 125, Loss : 1.7761588096618652\n",
            "Batch : 126, Loss : 1.7065585851669312\n",
            "Batch : 127, Loss : 1.6908060312271118\n",
            "Batch : 128, Loss : 1.7021220922470093\n",
            "Batch : 129, Loss : 1.6924667358398438\n",
            "Batch : 130, Loss : 1.7173854112625122\n",
            "Batch : 131, Loss : 1.828930377960205\n",
            "Batch : 132, Loss : 1.7294394969940186\n",
            "Batch : 133, Loss : 1.6882712841033936\n",
            "Batch : 134, Loss : 1.6525688171386719\n",
            "Batch : 135, Loss : 1.7551229000091553\n",
            "Batch : 136, Loss : 1.5355976819992065\n",
            "Batch : 137, Loss : 1.646278977394104\n",
            "Batch : 138, Loss : 1.6465942859649658\n",
            "Batch : 139, Loss : 1.6634668111801147\n",
            "Batch : 140, Loss : 1.6137865781784058\n",
            "Batch : 141, Loss : 1.6068750619888306\n",
            "Batch : 142, Loss : 1.5990670919418335\n",
            "Batch : 143, Loss : 1.549269676208496\n",
            "Batch : 144, Loss : 1.612372636795044\n",
            "Batch : 145, Loss : 1.6153498888015747\n",
            "Batch : 146, Loss : 1.5662177801132202\n",
            "Batch : 147, Loss : 1.6139767169952393\n",
            "Batch : 148, Loss : 1.571598768234253\n",
            "Batch : 149, Loss : 1.5634262561798096\n",
            "Batch : 150, Loss : 1.6037116050720215\n",
            "Batch : 151, Loss : 1.5231215953826904\n",
            "Batch : 152, Loss : 1.58145010471344\n",
            "Batch : 153, Loss : 1.4940907955169678\n",
            "Batch : 154, Loss : 1.570279598236084\n",
            "Batch : 155, Loss : 1.5212146043777466\n",
            "Batch : 156, Loss : 1.4687111377716064\n",
            "Batch : 157, Loss : 1.5223448276519775\n",
            "Batch : 158, Loss : 1.5174373388290405\n",
            "Batch : 159, Loss : 1.5285216569900513\n",
            "Batch : 160, Loss : 1.412540316581726\n",
            "Batch : 161, Loss : 1.523037075996399\n",
            "Batch : 162, Loss : 1.46878981590271\n",
            "Batch : 163, Loss : 1.4655696153640747\n",
            "Batch : 164, Loss : 1.5531837940216064\n",
            "Batch : 165, Loss : 1.363665223121643\n",
            "Batch : 166, Loss : 1.4294335842132568\n",
            "Batch : 167, Loss : 1.530592918395996\n",
            "Batch : 168, Loss : 1.4400734901428223\n",
            "Batch : 169, Loss : 1.5289416313171387\n",
            "Batch : 170, Loss : 1.333319067955017\n",
            "Batch : 171, Loss : 1.3673577308654785\n",
            "Batch : 172, Loss : 1.477495789527893\n",
            "Batch : 173, Loss : 1.402530312538147\n",
            "Batch : 174, Loss : 1.3398621082305908\n",
            "Batch : 175, Loss : 1.5014078617095947\n",
            "Batch : 176, Loss : 1.4087949991226196\n",
            "Batch : 177, Loss : 1.3808907270431519\n",
            "Batch : 178, Loss : 1.3921406269073486\n",
            "Batch : 179, Loss : 1.3133984804153442\n",
            "Batch : 180, Loss : 1.423866868019104\n",
            "Batch : 181, Loss : 1.3105669021606445\n",
            "Batch : 182, Loss : 1.4255033731460571\n",
            "Batch : 183, Loss : 1.2454816102981567\n",
            "Batch : 184, Loss : 1.2572320699691772\n",
            "Batch : 185, Loss : 1.4544849395751953\n",
            "Batch : 186, Loss : 1.4189144372940063\n",
            "Batch : 187, Loss : 1.3786885738372803\n",
            "Batch : 188, Loss : 1.3173240423202515\n",
            "Batch : 189, Loss : 1.3588656187057495\n",
            "Batch : 190, Loss : 1.3300764560699463\n",
            "Batch : 191, Loss : 1.39619779586792\n",
            "Batch : 192, Loss : 1.3493255376815796\n",
            "Batch : 193, Loss : 1.3257120847702026\n",
            "Batch : 194, Loss : 1.364181637763977\n",
            "Batch : 195, Loss : 1.2207283973693848\n",
            "Batch : 196, Loss : 1.2762483358383179\n",
            "Batch : 197, Loss : 1.2875434160232544\n",
            "Batch : 198, Loss : 1.354750156402588\n",
            "Batch : 199, Loss : 1.318312168121338\n",
            "Batch : 200, Loss : 1.2830575704574585\n",
            "Batch : 201, Loss : 1.2942707538604736\n",
            "Batch : 202, Loss : 1.2417126893997192\n",
            "Batch : 203, Loss : 1.2260706424713135\n",
            "Batch : 204, Loss : 1.241748571395874\n",
            "Batch : 205, Loss : 1.357450008392334\n",
            "Batch : 206, Loss : 1.3343126773834229\n",
            "Batch : 207, Loss : 1.3682434558868408\n",
            "Batch : 208, Loss : 1.2734856605529785\n",
            "Batch : 209, Loss : 1.1866130828857422\n",
            "Batch : 210, Loss : 1.1641842126846313\n",
            "Batch : 211, Loss : 1.1499658823013306\n",
            "Batch : 212, Loss : 1.2440028190612793\n",
            "Batch : 213, Loss : 1.1706194877624512\n",
            "Batch : 214, Loss : 1.174830436706543\n",
            "Batch : 215, Loss : 1.1569327116012573\n",
            "Batch : 216, Loss : 1.2802579402923584\n",
            "Batch : 217, Loss : 1.177161455154419\n",
            "Batch : 218, Loss : 1.1578035354614258\n",
            "Batch : 219, Loss : 1.2280371189117432\n",
            "Batch : 220, Loss : 1.180006980895996\n",
            "Batch : 221, Loss : 1.226579189300537\n",
            "Batch : 222, Loss : 1.267601490020752\n",
            "Batch : 223, Loss : 1.1983592510223389\n",
            "Batch : 224, Loss : 1.1959627866744995\n",
            "Batch : 225, Loss : 1.1601349115371704\n",
            "Batch : 226, Loss : 1.1433310508728027\n",
            "Batch : 227, Loss : 1.251500129699707\n",
            "Batch : 228, Loss : 1.1431307792663574\n",
            "Batch : 229, Loss : 1.122124433517456\n",
            "Batch : 230, Loss : 1.1665568351745605\n",
            "Batch : 231, Loss : 1.09718656539917\n",
            "Batch : 232, Loss : 1.1552088260650635\n",
            "Batch : 233, Loss : 1.1810898780822754\n",
            "Batch : 234, Loss : 1.0413883924484253\n",
            "Batch : 235, Loss : 1.1132420301437378\n",
            "Batch : 236, Loss : 1.0657382011413574\n",
            "Batch : 237, Loss : 1.1258310079574585\n",
            "Batch : 238, Loss : 1.1661055088043213\n",
            "Batch : 239, Loss : 1.1559984683990479\n",
            "Batch : 240, Loss : 1.1908355951309204\n",
            "Batch : 241, Loss : 1.015283465385437\n",
            "Batch : 242, Loss : 1.1262826919555664\n",
            "Batch : 243, Loss : 1.1268223524093628\n",
            "Batch : 244, Loss : 1.057759165763855\n",
            "Batch : 245, Loss : 1.172175407409668\n",
            "Batch : 246, Loss : 1.2536020278930664\n",
            "Batch : 247, Loss : 1.1177990436553955\n",
            "Batch : 248, Loss : 0.9556705951690674\n",
            "Batch : 249, Loss : 1.1890673637390137\n",
            "Batch : 250, Loss : 0.9430696368217468\n",
            "Batch : 251, Loss : 1.2418887615203857\n",
            "Batch : 252, Loss : 1.0399521589279175\n",
            "Batch : 253, Loss : 1.026871919631958\n",
            "Batch : 254, Loss : 1.122694492340088\n",
            "Batch : 255, Loss : 0.958888053894043\n",
            "Batch : 256, Loss : 1.0482428073883057\n",
            "Batch : 257, Loss : 1.0536381006240845\n",
            "Batch : 258, Loss : 0.9633442759513855\n",
            "Batch : 259, Loss : 0.9027984738349915\n",
            "Batch : 260, Loss : 1.047760009765625\n",
            "Batch : 261, Loss : 1.0276318788528442\n",
            "Batch : 262, Loss : 1.036224603652954\n",
            "Batch : 263, Loss : 1.041273593902588\n",
            "Batch : 264, Loss : 1.0915523767471313\n",
            "Batch : 265, Loss : 1.0031007528305054\n",
            "Batch : 266, Loss : 1.0471007823944092\n",
            "Batch : 267, Loss : 1.023746132850647\n",
            "Batch : 268, Loss : 0.9736934304237366\n",
            "Batch : 269, Loss : 1.0728448629379272\n",
            "Batch : 270, Loss : 1.12032949924469\n",
            "Batch : 271, Loss : 1.1260650157928467\n",
            "Batch : 272, Loss : 0.9762080311775208\n",
            "Batch : 273, Loss : 0.9753857851028442\n",
            "Batch : 274, Loss : 1.080201506614685\n",
            "Batch : 275, Loss : 0.9871788620948792\n",
            "Batch : 276, Loss : 0.921271562576294\n",
            "Batch : 277, Loss : 1.0659838914871216\n",
            "Batch : 278, Loss : 1.0326862335205078\n",
            "Batch : 279, Loss : 0.8639193773269653\n",
            "Batch : 280, Loss : 0.9340468049049377\n",
            "Batch : 281, Loss : 0.9103538990020752\n",
            "Batch : 282, Loss : 1.0290143489837646\n",
            "Batch : 283, Loss : 1.1040918827056885\n",
            "Batch : 284, Loss : 0.9259893894195557\n",
            "Batch : 285, Loss : 1.150700330734253\n",
            "Batch : 286, Loss : 0.9976696968078613\n",
            "Batch : 287, Loss : 1.0245956182479858\n",
            "Batch : 288, Loss : 0.9781970977783203\n",
            "Batch : 289, Loss : 0.9709538817405701\n",
            "Batch : 290, Loss : 0.9624162316322327\n",
            "Batch : 291, Loss : 0.9086658954620361\n",
            "Batch : 292, Loss : 0.9220131635665894\n",
            "Batch : 293, Loss : 1.1071761846542358\n",
            "Batch : 294, Loss : 0.9951992034912109\n",
            "Batch : 295, Loss : 1.0453171730041504\n",
            "Batch : 296, Loss : 0.9700947403907776\n",
            "Batch : 297, Loss : 0.9812923669815063\n",
            "Batch : 298, Loss : 0.8443306088447571\n",
            "Batch : 299, Loss : 1.0257091522216797\n",
            "Batch : 300, Loss : 0.92180997133255\n",
            "Batch : 301, Loss : 1.031323790550232\n",
            "Batch : 302, Loss : 0.9715271592140198\n",
            "Batch : 303, Loss : 1.0784144401550293\n",
            "Batch : 304, Loss : 0.8974742889404297\n",
            "Batch : 305, Loss : 0.9881243109703064\n",
            "Batch : 306, Loss : 0.8744966387748718\n",
            "Batch : 307, Loss : 1.0306466817855835\n",
            "Batch : 308, Loss : 1.001482605934143\n",
            "Batch : 309, Loss : 0.9778922200202942\n",
            "Batch : 310, Loss : 0.9148138165473938\n",
            "Batch : 311, Loss : 0.861985445022583\n",
            "Batch : 312, Loss : 0.9573820233345032\n",
            "Batch : 313, Loss : 1.0129903554916382\n",
            "Batch : 314, Loss : 1.0384464263916016\n",
            "Batch : 315, Loss : 0.8988887667655945\n",
            "Batch : 316, Loss : 0.9788470268249512\n",
            "Batch : 317, Loss : 0.9914155602455139\n",
            "Batch : 318, Loss : 0.8891151547431946\n",
            "Batch : 319, Loss : 0.8421124815940857\n",
            "Batch : 320, Loss : 1.0095983743667603\n",
            "Batch : 321, Loss : 0.9023210406303406\n",
            "Batch : 322, Loss : 1.0384455919265747\n",
            "Batch : 323, Loss : 0.8464905619621277\n",
            "Batch : 324, Loss : 0.9050508737564087\n",
            "Batch : 325, Loss : 0.8597528338432312\n",
            "Batch : 326, Loss : 1.0005767345428467\n",
            "Batch : 327, Loss : 0.8028961420059204\n",
            "Batch : 328, Loss : 1.0700253248214722\n",
            "Batch : 329, Loss : 0.9034140706062317\n",
            "Batch : 330, Loss : 1.0526052713394165\n",
            "Batch : 331, Loss : 0.9323996305465698\n",
            "Batch : 332, Loss : 0.7190316915512085\n",
            "Batch : 333, Loss : 0.9061477780342102\n",
            "Batch : 334, Loss : 0.9383627772331238\n",
            "Batch : 335, Loss : 0.7935499548912048\n",
            "Batch : 336, Loss : 0.8301109075546265\n",
            "Batch : 337, Loss : 0.9523021578788757\n",
            "Batch : 338, Loss : 1.0211318731307983\n",
            "Batch : 339, Loss : 0.8433555364608765\n",
            "Batch : 340, Loss : 0.7848140001296997\n",
            "Batch : 341, Loss : 0.8723663687705994\n",
            "Batch : 342, Loss : 0.7933780550956726\n",
            "Batch : 343, Loss : 0.7873713970184326\n",
            "Batch : 344, Loss : 0.96981281042099\n",
            "Batch : 345, Loss : 0.8194624185562134\n",
            "Batch : 346, Loss : 0.9096817970275879\n",
            "Batch : 347, Loss : 1.0181275606155396\n",
            "Batch : 348, Loss : 0.8567928671836853\n",
            "Batch : 349, Loss : 0.9397850632667542\n",
            "Batch : 350, Loss : 0.7980963587760925\n",
            "Batch : 351, Loss : 0.836601972579956\n",
            "Batch : 352, Loss : 0.8065356016159058\n",
            "Batch : 353, Loss : 0.7830615639686584\n",
            "Batch : 354, Loss : 0.8413993716239929\n",
            "Batch : 355, Loss : 0.8168015480041504\n",
            "Batch : 356, Loss : 0.9013144969940186\n",
            "Batch : 357, Loss : 0.8086021542549133\n",
            "Batch : 358, Loss : 1.017616868019104\n",
            "Batch : 359, Loss : 0.8533865809440613\n",
            "Batch : 360, Loss : 0.9138941764831543\n",
            "Batch : 361, Loss : 0.81352299451828\n",
            "Batch : 362, Loss : 0.867861807346344\n",
            "Batch : 363, Loss : 0.8142474889755249\n",
            "Batch : 364, Loss : 0.7950587868690491\n",
            "Batch : 365, Loss : 0.9438130855560303\n",
            "Batch : 366, Loss : 0.84843510389328\n",
            "Batch : 367, Loss : 0.9820589423179626\n",
            "Batch : 368, Loss : 0.8435500860214233\n",
            "Batch : 369, Loss : 0.9194377660751343\n",
            "Batch : 370, Loss : 0.7167849540710449\n",
            "Batch : 371, Loss : 0.9516265392303467\n",
            "Batch : 372, Loss : 0.7716405987739563\n",
            "Batch : 373, Loss : 0.9193195700645447\n",
            "Batch : 374, Loss : 0.9354747533798218\n",
            "Batch : 375, Loss : 0.8145610690116882\n",
            "Batch : 376, Loss : 0.8570785522460938\n",
            "Batch : 377, Loss : 0.8187587261199951\n",
            "Batch : 378, Loss : 0.7502798438072205\n",
            "Batch : 379, Loss : 0.7492271661758423\n",
            "Batch : 380, Loss : 0.9412727952003479\n",
            "Batch : 381, Loss : 0.8338159322738647\n",
            "Batch : 382, Loss : 0.9402385354042053\n",
            "Batch : 383, Loss : 0.8287283182144165\n",
            "Batch : 384, Loss : 0.8959277272224426\n",
            "Batch : 385, Loss : 0.8354538679122925\n",
            "Batch : 386, Loss : 0.8579855561256409\n",
            "Batch : 387, Loss : 0.8460618853569031\n",
            "Batch : 388, Loss : 0.8218604922294617\n",
            "Batch : 389, Loss : 0.7780641317367554\n",
            "Batch : 390, Loss : 0.8712314963340759\n",
            "Batch : 391, Loss : 0.8644263744354248\n",
            "Batch : 392, Loss : 0.9746073484420776\n",
            "Batch : 393, Loss : 0.6903963685035706\n",
            "Batch : 394, Loss : 0.815913736820221\n",
            "Batch : 395, Loss : 1.0024220943450928\n",
            "Batch : 396, Loss : 0.979579746723175\n",
            "Batch : 397, Loss : 0.7999640703201294\n",
            "Batch : 398, Loss : 0.8245099186897278\n",
            "Batch : 399, Loss : 0.7921676635742188\n",
            "Batch : 400, Loss : 0.8056285381317139\n",
            "Batch : 401, Loss : 0.8696582317352295\n",
            "Batch : 402, Loss : 1.008313775062561\n",
            "Batch : 403, Loss : 0.821925938129425\n",
            "Batch : 404, Loss : 0.7236857414245605\n",
            "Batch : 405, Loss : 0.8378642201423645\n",
            "Batch : 406, Loss : 0.8451458215713501\n",
            "Batch : 407, Loss : 0.8220269083976746\n",
            "Batch : 408, Loss : 0.8544408082962036\n",
            "Batch : 409, Loss : 0.8947476148605347\n",
            "Batch : 410, Loss : 0.8523430824279785\n",
            "Batch : 411, Loss : 0.686777651309967\n",
            "Batch : 412, Loss : 0.7630425691604614\n",
            "Batch : 413, Loss : 0.7564067244529724\n",
            "Batch : 414, Loss : 0.8719900846481323\n",
            "Batch : 415, Loss : 0.7124180793762207\n",
            "Batch : 416, Loss : 0.6385090351104736\n",
            "Batch : 417, Loss : 0.905329704284668\n",
            "Batch : 418, Loss : 0.7619565725326538\n",
            "Batch : 419, Loss : 0.7236204743385315\n",
            "Batch : 420, Loss : 0.6218743920326233\n",
            "Batch : 421, Loss : 0.8735451102256775\n",
            "Batch : 422, Loss : 0.8710263967514038\n",
            "Batch : 423, Loss : 0.6975619196891785\n",
            "Batch : 424, Loss : 0.7696322798728943\n",
            "Batch : 425, Loss : 0.708920419216156\n",
            "Batch : 426, Loss : 0.957980751991272\n",
            "Batch : 427, Loss : 0.8502087593078613\n",
            "Batch : 428, Loss : 0.6800288558006287\n",
            "Batch : 429, Loss : 0.6577925682067871\n",
            "Batch : 430, Loss : 0.85988450050354\n",
            "Batch : 431, Loss : 0.6876816749572754\n",
            "Batch : 432, Loss : 0.755408525466919\n",
            "Batch : 433, Loss : 0.8221731781959534\n",
            "Batch : 434, Loss : 0.6499769687652588\n",
            "Batch : 435, Loss : 0.7069469094276428\n",
            "Batch : 436, Loss : 0.9539620876312256\n",
            "Batch : 437, Loss : 0.9023582935333252\n",
            "Batch : 438, Loss : 0.7716598510742188\n",
            "Batch : 439, Loss : 0.777564525604248\n",
            "Batch : 440, Loss : 0.8212223649024963\n",
            "Batch : 441, Loss : 0.6453914046287537\n",
            "Batch : 442, Loss : 0.7416077852249146\n",
            "Batch : 443, Loss : 1.0310521125793457\n",
            "Batch : 444, Loss : 0.64264315366745\n",
            "Batch : 445, Loss : 0.9429836869239807\n",
            "Batch : 446, Loss : 0.6142268180847168\n",
            "Batch : 447, Loss : 0.8133031725883484\n",
            "Batch : 448, Loss : 0.7290821075439453\n",
            "Batch : 449, Loss : 0.748490571975708\n",
            "Batch : 450, Loss : 1.0006179809570312\n",
            "Batch : 451, Loss : 0.7622305750846863\n",
            "Batch : 452, Loss : 0.8721713423728943\n",
            "Batch : 453, Loss : 0.8406214714050293\n",
            "Batch : 454, Loss : 0.8123415112495422\n",
            "Batch : 455, Loss : 0.7909340858459473\n",
            "Batch : 456, Loss : 0.7534286975860596\n",
            "Batch : 457, Loss : 0.8640386462211609\n",
            "Batch : 458, Loss : 0.7669526934623718\n",
            "Batch : 459, Loss : 0.697235643863678\n",
            "Batch : 460, Loss : 0.7151793837547302\n",
            "Batch : 461, Loss : 0.714285135269165\n",
            "Batch : 462, Loss : 0.7664349675178528\n",
            "Batch : 463, Loss : 0.8185322284698486\n",
            "Batch : 464, Loss : 0.7587229609489441\n",
            "Batch : 465, Loss : 0.7474848628044128\n",
            "Batch : 466, Loss : 0.7164430618286133\n",
            "Batch : 467, Loss : 0.7266503572463989\n",
            "Batch : 468, Loss : 0.8036649823188782\n",
            "Batch : 469, Loss : 0.7027947306632996\n",
            "Batch : 470, Loss : 0.6876162886619568\n",
            "Batch : 471, Loss : 0.6561566591262817\n",
            "Batch : 472, Loss : 0.7451842427253723\n",
            "Batch : 473, Loss : 0.6233623027801514\n",
            "Batch : 474, Loss : 1.024216651916504\n",
            "Batch : 475, Loss : 0.7085868716239929\n",
            "Batch : 476, Loss : 0.7588778734207153\n",
            "Batch : 477, Loss : 0.7437729835510254\n",
            "Batch : 478, Loss : 0.8223629593849182\n",
            "Batch : 479, Loss : 0.8037599921226501\n",
            "Batch : 480, Loss : 0.7843420505523682\n",
            "Batch : 481, Loss : 0.7188528180122375\n",
            "Batch : 482, Loss : 0.9432944655418396\n",
            "Batch : 483, Loss : 0.6095778942108154\n",
            "Batch : 484, Loss : 0.809231698513031\n",
            "Batch : 485, Loss : 0.9115051627159119\n",
            "Batch : 486, Loss : 0.9219542145729065\n",
            "Batch : 487, Loss : 0.8695440292358398\n",
            "Batch : 488, Loss : 0.7691060304641724\n",
            "Batch : 489, Loss : 0.8392326235771179\n",
            "Batch : 490, Loss : 0.9045084714889526\n",
            "Batch : 491, Loss : 0.6989932656288147\n",
            "Batch : 492, Loss : 0.7305490970611572\n",
            "Batch : 493, Loss : 0.6524064540863037\n",
            "Batch : 494, Loss : 0.7411080598831177\n",
            "Batch : 495, Loss : 0.7518683075904846\n",
            "Batch : 496, Loss : 0.6646929979324341\n",
            "Batch : 497, Loss : 0.9094655513763428\n",
            "Batch : 498, Loss : 0.9493762850761414\n",
            "Batch : 499, Loss : 0.5900125503540039\n",
            "Batch : 500, Loss : 0.7292956113815308\n",
            "Batch : 501, Loss : 0.8917055130004883\n",
            "Batch : 502, Loss : 0.7111058831214905\n",
            "Batch : 503, Loss : 0.9055768847465515\n",
            "Batch : 504, Loss : 0.7318176627159119\n",
            "Batch : 505, Loss : 0.7245761156082153\n",
            "Batch : 506, Loss : 0.6376231908798218\n",
            "Batch : 507, Loss : 0.6962372660636902\n",
            "Batch : 508, Loss : 0.6461793780326843\n",
            "Batch : 509, Loss : 0.7605994343757629\n",
            "Batch : 510, Loss : 0.6399065256118774\n",
            "Batch : 511, Loss : 0.5486250519752502\n",
            "Batch : 512, Loss : 0.6086750626564026\n",
            "Batch : 513, Loss : 0.7511720657348633\n",
            "Batch : 514, Loss : 0.7772237062454224\n",
            "Batch : 515, Loss : 0.8357182741165161\n",
            "Batch : 516, Loss : 0.6972845792770386\n",
            "Batch : 517, Loss : 0.6551563739776611\n",
            "Batch : 518, Loss : 0.885707676410675\n",
            "Batch : 519, Loss : 0.6553475260734558\n",
            "Batch : 520, Loss : 0.7293595671653748\n",
            "Batch : 521, Loss : 0.8831605911254883\n",
            "Batch : 522, Loss : 0.9499047994613647\n",
            "Batch : 523, Loss : 0.7569442987442017\n",
            "Batch : 524, Loss : 0.6278385519981384\n",
            "Batch : 525, Loss : 0.5940654277801514\n",
            "Batch : 526, Loss : 0.8703826665878296\n",
            "Batch : 527, Loss : 0.7622965574264526\n",
            "Batch : 528, Loss : 0.8508467078208923\n",
            "Batch : 529, Loss : 0.6475473046302795\n",
            "Batch : 530, Loss : 0.7453390955924988\n",
            "Batch : 531, Loss : 0.8192614912986755\n",
            "Batch : 532, Loss : 0.8034867644309998\n",
            "Batch : 533, Loss : 0.7156414985656738\n",
            "Batch : 534, Loss : 0.9004601836204529\n",
            "Batch : 535, Loss : 0.6994414329528809\n",
            "Batch : 536, Loss : 0.6783607006072998\n",
            "Batch : 537, Loss : 0.5105931758880615\n",
            "Batch : 538, Loss : 0.6916530132293701\n",
            "Batch : 539, Loss : 0.7968937754631042\n",
            "Batch : 540, Loss : 0.6853781938552856\n",
            "Batch : 541, Loss : 0.5497510433197021\n",
            "Batch : 542, Loss : 0.656327486038208\n",
            "Batch : 543, Loss : 0.8119845390319824\n",
            "Batch : 544, Loss : 0.7034465670585632\n",
            "Batch : 545, Loss : 0.7863984704017639\n",
            "Batch : 546, Loss : 0.6656404137611389\n",
            "Batch : 547, Loss : 0.5760912299156189\n",
            "Batch : 548, Loss : 0.5988243222236633\n",
            "Batch : 549, Loss : 0.6492378115653992\n",
            "Batch : 550, Loss : 0.9244956970214844\n",
            "Batch : 551, Loss : 0.7666069269180298\n",
            "Batch : 552, Loss : 0.6802918314933777\n",
            "Batch : 553, Loss : 0.8263715505599976\n",
            "Batch : 554, Loss : 0.6526376605033875\n",
            "Batch : 555, Loss : 0.7449937462806702\n",
            "Batch : 556, Loss : 0.7429196238517761\n",
            "Batch : 557, Loss : 0.7299710512161255\n",
            "Batch : 558, Loss : 0.760004997253418\n",
            "Batch : 559, Loss : 0.738767683506012\n",
            "Batch : 560, Loss : 0.6839049458503723\n",
            "Batch : 561, Loss : 0.7147824764251709\n",
            "Batch : 562, Loss : 0.6360207200050354\n",
            "Batch : 563, Loss : 0.746308445930481\n",
            "Batch : 564, Loss : 0.749696671962738\n",
            "Batch : 565, Loss : 0.7229093909263611\n",
            "Batch : 566, Loss : 0.7618407607078552\n",
            "Batch : 567, Loss : 0.7319571375846863\n",
            "Batch : 568, Loss : 0.8861110210418701\n",
            "Batch : 569, Loss : 0.7592527866363525\n",
            "Batch : 570, Loss : 0.8594906330108643\n",
            "Batch : 571, Loss : 0.6393920183181763\n",
            "Batch : 572, Loss : 0.7068223357200623\n",
            "Batch : 573, Loss : 0.7009856700897217\n",
            "Batch : 574, Loss : 0.7143883109092712\n",
            "Batch : 575, Loss : 0.7381808161735535\n",
            "Batch : 576, Loss : 0.9086108803749084\n",
            "Batch : 577, Loss : 0.6745588779449463\n",
            "Batch : 578, Loss : 0.7607182860374451\n",
            "Batch : 579, Loss : 0.6843788027763367\n",
            "Batch : 580, Loss : 0.5348325371742249\n",
            "Batch : 581, Loss : 0.8006631135940552\n",
            "Batch : 582, Loss : 0.8183212280273438\n",
            "Batch : 583, Loss : 0.5916959047317505\n",
            "Batch : 584, Loss : 0.6697779893875122\n",
            "Batch : 585, Loss : 0.6426995992660522\n",
            "Batch : 586, Loss : 0.6925416588783264\n",
            "Batch : 587, Loss : 0.688721239566803\n",
            "Batch : 588, Loss : 0.6599916219711304\n",
            "Batch : 589, Loss : 0.6264340877532959\n",
            "Batch : 590, Loss : 0.8691897988319397\n",
            "Batch : 591, Loss : 0.5763455629348755\n",
            "Batch : 592, Loss : 0.8746542930603027\n",
            "Batch : 593, Loss : 0.6220310926437378\n",
            "Batch : 594, Loss : 0.4921497702598572\n",
            "Batch : 595, Loss : 0.7196513414382935\n",
            "Batch : 596, Loss : 0.8423633575439453\n",
            "Batch : 597, Loss : 0.6069219708442688\n",
            "Batch : 598, Loss : 0.6070460081100464\n",
            "Batch : 599, Loss : 0.7218278050422668\n",
            "Batch : 600, Loss : 0.6278942823410034\n",
            "Batch : 601, Loss : 0.7477478981018066\n",
            "Batch : 602, Loss : 0.5905880331993103\n",
            "Batch : 603, Loss : 0.5792530179023743\n",
            "Batch : 604, Loss : 0.696749210357666\n",
            "Batch : 605, Loss : 0.8390598297119141\n",
            "Batch : 606, Loss : 0.5644471049308777\n",
            "Batch : 607, Loss : 0.5655978918075562\n",
            "Batch : 608, Loss : 0.6638390421867371\n",
            "Batch : 609, Loss : 0.6166312098503113\n",
            "Batch : 610, Loss : 0.7301591634750366\n",
            "Batch : 611, Loss : 0.6034293174743652\n",
            "Batch : 612, Loss : 0.706328809261322\n",
            "Batch : 613, Loss : 0.6729917526245117\n",
            "Batch : 614, Loss : 0.5959288477897644\n",
            "Batch : 615, Loss : 0.5431894063949585\n",
            "Batch : 616, Loss : 0.645757794380188\n",
            "Batch : 617, Loss : 0.8167617321014404\n",
            "Batch : 618, Loss : 0.802730143070221\n",
            "Batch : 619, Loss : 0.8363057971000671\n",
            "Batch : 620, Loss : 0.6841832399368286\n",
            "Batch : 621, Loss : 0.7409456968307495\n",
            "Batch : 622, Loss : 0.5572080612182617\n",
            "Batch : 623, Loss : 0.6593300104141235\n",
            "Batch : 624, Loss : 0.795583963394165\n",
            "Batch : 625, Loss : 0.6263511776924133\n",
            "Batch : 626, Loss : 0.6702433824539185\n",
            "Batch : 627, Loss : 0.581233024597168\n",
            "Batch : 628, Loss : 0.6089920997619629\n",
            "Batch : 629, Loss : 0.601514995098114\n",
            "Batch : 630, Loss : 0.7224923372268677\n",
            "Batch : 631, Loss : 0.7291979193687439\n",
            "Batch : 632, Loss : 0.578123152256012\n",
            "Batch : 633, Loss : 0.6445505023002625\n",
            "Batch : 634, Loss : 0.6345677375793457\n",
            "Batch : 635, Loss : 0.7515628933906555\n",
            "Batch : 636, Loss : 0.767900288105011\n",
            "Batch : 637, Loss : 0.6441702246665955\n",
            "Batch : 638, Loss : 0.6960898637771606\n",
            "Batch : 639, Loss : 0.6463536024093628\n",
            "Batch : 640, Loss : 0.7103933691978455\n",
            "Batch : 641, Loss : 0.7670366764068604\n",
            "Batch : 642, Loss : 0.7376696467399597\n",
            "Batch : 643, Loss : 0.6627037525177002\n",
            "Batch : 644, Loss : 0.7628393173217773\n",
            "Batch : 645, Loss : 0.6822605133056641\n",
            "Batch : 646, Loss : 0.764560878276825\n",
            "Batch : 647, Loss : 0.8093379139900208\n",
            "Batch : 648, Loss : 0.7715760469436646\n",
            "Batch : 649, Loss : 0.6206375360488892\n",
            "Batch : 650, Loss : 0.6242222785949707\n",
            "Batch : 651, Loss : 0.6169900894165039\n",
            "Batch : 652, Loss : 0.6791025400161743\n",
            "Batch : 653, Loss : 0.8996403217315674\n",
            "Batch : 654, Loss : 0.9351795315742493\n",
            "Batch : 655, Loss : 0.7237681150436401\n",
            "Batch : 656, Loss : 0.6139572262763977\n",
            "Batch : 657, Loss : 0.7969054579734802\n",
            "Batch : 658, Loss : 0.7169980406761169\n",
            "Batch : 659, Loss : 0.8346990346908569\n",
            "Batch : 660, Loss : 0.8385842442512512\n",
            "Batch : 661, Loss : 0.690051257610321\n",
            "Batch : 662, Loss : 0.5257539749145508\n",
            "Batch : 663, Loss : 0.660486102104187\n",
            "Batch : 664, Loss : 0.6547372937202454\n",
            "Batch : 665, Loss : 0.6994713544845581\n",
            "Batch : 666, Loss : 0.6079408526420593\n",
            "Batch : 667, Loss : 0.6338478326797485\n",
            "Batch : 668, Loss : 0.6278578639030457\n",
            "Batch : 669, Loss : 0.7325486540794373\n",
            "Batch : 670, Loss : 0.7443001866340637\n",
            "Batch : 671, Loss : 0.570955216884613\n",
            "Batch : 672, Loss : 0.6993566155433655\n",
            "Batch : 673, Loss : 0.5940546989440918\n",
            "Batch : 674, Loss : 0.6609767079353333\n",
            "Batch : 675, Loss : 0.7274627685546875\n",
            "Batch : 676, Loss : 0.7595685720443726\n",
            "Batch : 677, Loss : 0.7219184637069702\n",
            "Batch : 678, Loss : 0.7027735710144043\n",
            "Batch : 679, Loss : 0.8414350152015686\n",
            "Batch : 680, Loss : 0.713499903678894\n",
            "Batch : 681, Loss : 0.7225098609924316\n",
            "Batch : 682, Loss : 0.8063880801200867\n",
            "Batch : 683, Loss : 0.5072847008705139\n",
            "Batch : 684, Loss : 0.6596932411193848\n",
            "Batch : 685, Loss : 0.7605757117271423\n",
            "Batch : 686, Loss : 0.8141723275184631\n",
            "Batch : 687, Loss : 0.7441927790641785\n",
            "Batch : 688, Loss : 0.6967683434486389\n",
            "Batch : 689, Loss : 0.8732810020446777\n",
            "Batch : 690, Loss : 0.6614017486572266\n",
            "Batch : 691, Loss : 0.5966011881828308\n",
            "Batch : 692, Loss : 0.5783404111862183\n",
            "Batch : 693, Loss : 0.608776867389679\n",
            "Batch : 694, Loss : 0.704170286655426\n",
            "Batch : 695, Loss : 0.7105695605278015\n",
            "Batch : 696, Loss : 0.7039583325386047\n",
            "Batch : 697, Loss : 0.6004263758659363\n",
            "Batch : 698, Loss : 0.6791659593582153\n",
            "Batch : 699, Loss : 0.5627185702323914\n",
            "Batch : 700, Loss : 0.6062299609184265\n",
            "Batch : 701, Loss : 0.7108470797538757\n",
            "Batch : 702, Loss : 0.6133620142936707\n",
            "Batch : 703, Loss : 0.5731286406517029\n",
            "Batch : 704, Loss : 0.7120789289474487\n",
            "Batch : 705, Loss : 0.6691102385520935\n",
            "Batch : 706, Loss : 0.7390308976173401\n",
            "Batch : 707, Loss : 0.746967077255249\n",
            "Batch : 708, Loss : 0.49962443113327026\n",
            "Batch : 709, Loss : 0.4330023229122162\n",
            "Batch : 710, Loss : 0.6791303753852844\n",
            "Batch : 711, Loss : 0.6365832090377808\n",
            "Batch : 712, Loss : 0.7446313500404358\n",
            "Batch : 713, Loss : 0.5448007583618164\n",
            "Batch : 714, Loss : 0.8494369387626648\n",
            "Batch : 715, Loss : 0.5621843934059143\n",
            "Batch : 716, Loss : 0.7278045415878296\n",
            "Batch : 717, Loss : 0.6596109867095947\n",
            "Batch : 718, Loss : 0.7067063450813293\n",
            "Batch : 719, Loss : 0.5991939902305603\n",
            "Batch : 720, Loss : 0.6555167436599731\n",
            "Batch : 721, Loss : 0.6998389959335327\n",
            "Batch : 722, Loss : 0.9175878763198853\n",
            "Batch : 723, Loss : 0.6742823123931885\n",
            "Batch : 724, Loss : 0.5369758009910583\n",
            "Batch : 725, Loss : 0.5198394656181335\n",
            "Batch : 726, Loss : 0.6928083896636963\n",
            "Batch : 727, Loss : 0.5085330009460449\n",
            "Batch : 728, Loss : 0.5330448746681213\n",
            "Batch : 729, Loss : 0.606035053730011\n",
            "Batch : 730, Loss : 0.795789361000061\n",
            "Batch : 731, Loss : 0.6359867453575134\n",
            "Batch : 732, Loss : 0.61832195520401\n",
            "Batch : 733, Loss : 0.6779637932777405\n",
            "Batch : 734, Loss : 0.6540175080299377\n",
            "Batch : 735, Loss : 0.662041425704956\n",
            "Batch : 736, Loss : 0.7819855213165283\n",
            "Batch : 737, Loss : 0.790953516960144\n",
            "Batch : 738, Loss : 0.7292911410331726\n",
            "Batch : 739, Loss : 0.586837649345398\n",
            "Batch : 740, Loss : 0.7399391531944275\n",
            "Batch : 741, Loss : 0.6375719904899597\n",
            "Batch : 742, Loss : 0.7020020484924316\n",
            "Batch : 743, Loss : 0.8851290345191956\n",
            "Batch : 744, Loss : 0.6075869798660278\n",
            "Batch : 745, Loss : 0.5651993155479431\n",
            "Batch : 746, Loss : 0.643153190612793\n",
            "Batch : 747, Loss : 0.8169606924057007\n",
            "Batch : 748, Loss : 0.4936267137527466\n",
            "Batch : 749, Loss : 0.6529186964035034\n",
            "Batch : 750, Loss : 0.7056780457496643\n",
            "Batch : 751, Loss : 0.5667251944541931\n",
            "Batch : 752, Loss : 0.6627215147018433\n",
            "Batch : 753, Loss : 0.7234948873519897\n",
            "Batch : 754, Loss : 0.7780356407165527\n",
            "Batch : 755, Loss : 0.6112712621688843\n",
            "Batch : 756, Loss : 0.5461321473121643\n",
            "Batch : 757, Loss : 0.5463974475860596\n",
            "Batch : 758, Loss : 0.5439186096191406\n",
            "Batch : 759, Loss : 0.7420536279678345\n",
            "Batch : 760, Loss : 0.8565472960472107\n",
            "Batch : 761, Loss : 0.5648456811904907\n",
            "Batch : 762, Loss : 0.7242218255996704\n",
            "Batch : 763, Loss : 0.4457944631576538\n",
            "Batch : 764, Loss : 0.7598641514778137\n",
            "Batch : 765, Loss : 0.5873723030090332\n",
            "Batch : 766, Loss : 0.642180323600769\n",
            "Batch : 767, Loss : 0.4973856508731842\n",
            "Batch : 768, Loss : 0.594163715839386\n",
            "Batch : 769, Loss : 0.6530301570892334\n",
            "Batch : 770, Loss : 0.5186834335327148\n",
            "Batch : 771, Loss : 0.5561070442199707\n",
            "Batch : 772, Loss : 0.6307617425918579\n",
            "Batch : 773, Loss : 0.6522845029830933\n",
            "Batch : 774, Loss : 0.6012808680534363\n",
            "Batch : 775, Loss : 0.6504566669464111\n",
            "Batch : 776, Loss : 0.6418780088424683\n",
            "Batch : 777, Loss : 0.7317736148834229\n",
            "Batch : 778, Loss : 0.6317461729049683\n",
            "Batch : 779, Loss : 0.5804013609886169\n",
            "Batch : 780, Loss : 0.5857151746749878\n",
            "Batch : 781, Loss : 0.711887001991272\n",
            "Batch : 782, Loss : 0.5815554261207581\n",
            "Batch : 783, Loss : 0.6674370765686035\n",
            "Batch : 784, Loss : 0.4903400242328644\n",
            "Batch : 785, Loss : 0.6573659777641296\n",
            "Batch : 786, Loss : 0.5734623670578003\n",
            "Batch : 787, Loss : 0.6519448757171631\n",
            "Batch : 788, Loss : 0.6992300748825073\n",
            "Batch : 789, Loss : 0.5815191268920898\n",
            "Batch : 790, Loss : 0.7863885164260864\n",
            "Batch : 791, Loss : 0.5039893388748169\n",
            "Batch : 792, Loss : 0.637828528881073\n",
            "Batch : 793, Loss : 0.5277212262153625\n",
            "Batch : 794, Loss : 0.7077639102935791\n",
            "Batch : 795, Loss : 0.5875266790390015\n",
            "Batch : 796, Loss : 0.4449573755264282\n",
            "Batch : 797, Loss : 0.6045602560043335\n",
            "Batch : 798, Loss : 0.4870290160179138\n",
            "Batch : 799, Loss : 0.601201057434082\n",
            "Batch : 800, Loss : 0.70818692445755\n",
            "Batch : 801, Loss : 0.5501648187637329\n",
            "Batch : 802, Loss : 0.8017626404762268\n",
            "Batch : 803, Loss : 0.7492866516113281\n",
            "Batch : 804, Loss : 0.7418342232704163\n",
            "Batch : 805, Loss : 0.5900188088417053\n",
            "Batch : 806, Loss : 0.5484856963157654\n",
            "Batch : 807, Loss : 0.6147404909133911\n",
            "Batch : 808, Loss : 0.6540844440460205\n",
            "Batch : 809, Loss : 0.7327963709831238\n",
            "Batch : 810, Loss : 0.5103501081466675\n",
            "Batch : 811, Loss : 0.8667293190956116\n",
            "Batch : 812, Loss : 0.6219337582588196\n",
            "Batch : 813, Loss : 0.7742714881896973\n",
            "Batch : 814, Loss : 0.4966507852077484\n",
            "Batch : 815, Loss : 0.654079020023346\n",
            "Batch : 816, Loss : 0.778545618057251\n",
            "Batch : 817, Loss : 0.5346110463142395\n",
            "Batch : 818, Loss : 0.640964686870575\n",
            "Batch : 819, Loss : 0.465219110250473\n",
            "Batch : 820, Loss : 0.576029896736145\n",
            "Batch : 821, Loss : 0.710536003112793\n",
            "Batch : 822, Loss : 0.6724981665611267\n",
            "Batch : 823, Loss : 0.4503422677516937\n",
            "Batch : 824, Loss : 0.7158914804458618\n",
            "Batch : 825, Loss : 0.6532310247421265\n",
            "Batch : 826, Loss : 0.6108523011207581\n",
            "Batch : 827, Loss : 0.6557115316390991\n",
            "Batch : 828, Loss : 0.4444047808647156\n",
            "Batch : 829, Loss : 0.7303073406219482\n",
            "Batch : 830, Loss : 0.5679183006286621\n",
            "Batch : 831, Loss : 0.7941261529922485\n",
            "Batch : 832, Loss : 0.7880492806434631\n",
            "Batch : 833, Loss : 0.6714304685592651\n",
            "Batch : 834, Loss : 0.6314257383346558\n",
            "Batch : 835, Loss : 0.520862877368927\n",
            "Batch : 836, Loss : 0.600486695766449\n",
            "Batch : 837, Loss : 0.5241124033927917\n",
            "Batch : 838, Loss : 0.6494101285934448\n",
            "Batch : 839, Loss : 0.5850082635879517\n",
            "Batch : 840, Loss : 0.7026344537734985\n",
            "Batch : 841, Loss : 0.6248612999916077\n",
            "Batch : 842, Loss : 0.6224510073661804\n",
            "Batch : 843, Loss : 0.7425289154052734\n",
            "Batch : 844, Loss : 0.559794545173645\n",
            "Batch : 845, Loss : 0.6531394720077515\n",
            "Batch : 846, Loss : 0.581684410572052\n",
            "Batch : 847, Loss : 0.7060238718986511\n",
            "Batch : 848, Loss : 0.6265017986297607\n",
            "Batch : 849, Loss : 0.5346806645393372\n",
            "Batch : 850, Loss : 0.6422976851463318\n",
            "Batch : 851, Loss : 0.6553049683570862\n",
            "Batch : 852, Loss : 0.8034654259681702\n",
            "Batch : 853, Loss : 0.7234256267547607\n",
            "Batch : 854, Loss : 0.4335469901561737\n",
            "Batch : 855, Loss : 0.7045259475708008\n",
            "Batch : 856, Loss : 0.5153584480285645\n",
            "Batch : 857, Loss : 0.6894383430480957\n",
            "Batch : 858, Loss : 0.7014239430427551\n",
            "Batch : 859, Loss : 0.4579143226146698\n",
            "Batch : 860, Loss : 0.4808395802974701\n",
            "Batch : 861, Loss : 0.6943961977958679\n",
            "Batch : 862, Loss : 0.7178170680999756\n",
            "Batch : 863, Loss : 0.732236921787262\n",
            "Batch : 864, Loss : 0.6357458233833313\n",
            "Batch : 865, Loss : 0.5959268808364868\n",
            "Batch : 866, Loss : 0.5891640782356262\n",
            "Batch : 867, Loss : 0.6948224902153015\n",
            "Batch : 868, Loss : 0.5384396910667419\n",
            "Batch : 869, Loss : 0.7787744402885437\n",
            "Batch : 870, Loss : 0.681394100189209\n",
            "Batch : 871, Loss : 0.537460207939148\n",
            "Batch : 872, Loss : 0.5882447957992554\n",
            "Batch : 873, Loss : 0.5431686639785767\n",
            "Batch : 874, Loss : 0.47140172123908997\n",
            "Batch : 875, Loss : 0.6038120985031128\n",
            "Batch : 876, Loss : 0.5997838973999023\n",
            "Batch : 877, Loss : 0.6978808641433716\n",
            "Batch : 878, Loss : 0.5951569080352783\n",
            "Batch : 879, Loss : 0.5383782386779785\n",
            "Batch : 880, Loss : 0.5749527215957642\n",
            "Batch : 881, Loss : 0.5655680894851685\n",
            "Batch : 882, Loss : 0.6115674376487732\n",
            "Batch : 883, Loss : 0.7410445213317871\n",
            "Batch : 884, Loss : 0.5708023309707642\n",
            "Batch : 885, Loss : 0.7692174911499023\n",
            "Batch : 886, Loss : 0.7127276659011841\n",
            "Batch : 887, Loss : 0.7047935724258423\n",
            "Batch : 888, Loss : 0.8208912014961243\n",
            "Batch : 889, Loss : 0.5910651683807373\n",
            "Batch : 890, Loss : 0.5925623774528503\n",
            "Batch : 891, Loss : 0.5608049631118774\n",
            "Batch : 892, Loss : 0.4928632974624634\n",
            "Batch : 893, Loss : 0.7502812743186951\n",
            "Batch : 894, Loss : 0.5671312212944031\n",
            "Batch : 895, Loss : 0.5789173245429993\n",
            "Batch : 896, Loss : 0.4581032693386078\n",
            "Batch : 897, Loss : 0.5306779146194458\n",
            "Batch : 898, Loss : 0.8519716858863831\n",
            "Batch : 899, Loss : 0.6131786704063416\n",
            "Batch : 900, Loss : 0.623596727848053\n",
            "Batch : 901, Loss : 0.49338018894195557\n",
            "Batch : 902, Loss : 0.6859598755836487\n",
            "Batch : 903, Loss : 0.8164395093917847\n",
            "Batch : 904, Loss : 0.4315987527370453\n",
            "Batch : 905, Loss : 0.8421529531478882\n",
            "Batch : 906, Loss : 0.62263023853302\n",
            "Batch : 907, Loss : 0.6078515648841858\n",
            "Batch : 908, Loss : 0.4633856415748596\n",
            "Batch : 909, Loss : 0.8858580589294434\n",
            "Batch : 910, Loss : 0.7519479393959045\n",
            "Batch : 911, Loss : 0.7246900796890259\n",
            "Batch : 912, Loss : 0.5361359715461731\n",
            "Batch : 913, Loss : 0.5157343745231628\n",
            "Batch : 914, Loss : 0.5323681235313416\n",
            "Batch : 915, Loss : 0.4860877990722656\n",
            "Batch : 916, Loss : 0.7857773303985596\n",
            "Batch : 917, Loss : 0.5573807954788208\n",
            "Batch : 918, Loss : 0.7211037278175354\n",
            "Batch : 919, Loss : 0.5109787583351135\n",
            "Batch : 920, Loss : 0.6325426697731018\n",
            "Batch : 921, Loss : 0.7359611392021179\n",
            "Batch : 922, Loss : 0.6473660469055176\n",
            "Batch : 923, Loss : 0.6046074628829956\n",
            "Batch : 924, Loss : 0.5667253732681274\n",
            "Batch : 925, Loss : 0.551591694355011\n",
            "Batch : 926, Loss : 0.726132333278656\n",
            "Batch : 927, Loss : 0.6372841596603394\n",
            "Batch : 928, Loss : 0.5113666653633118\n",
            "Batch : 929, Loss : 0.5263793468475342\n",
            "Batch : 930, Loss : 0.7144997119903564\n",
            "Batch : 931, Loss : 0.8634676933288574\n",
            "Batch : 932, Loss : 0.4328876733779907\n",
            "Batch : 933, Loss : 0.5267865657806396\n",
            "Batch : 934, Loss : 0.5840389132499695\n",
            "Batch : 935, Loss : 0.37605488300323486\n",
            "Batch : 936, Loss : 0.6382399201393127\n",
            "Batch : 937, Loss : 0.6882227063179016\n",
            "Batch : 938, Loss : 0.59434574842453\n",
            "Training loss: 1.0015089905846601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nb d'images 60 000 , nb d'images par batch : 64 , nb de batvh  : 938"
      ],
      "metadata": {
        "id": "_S5V9GavURV8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWf1SWuiFGmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8207bd-b74e-4cca-f904-13ca88420dcb"
      },
      "source": [
        "60000/64"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "937.5"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_QUMXLFGjr"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    }
  ]
}